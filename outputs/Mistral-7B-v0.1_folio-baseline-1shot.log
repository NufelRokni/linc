ipex flag is deprecated, will be removed in Accelerate v1.10. From 2.7.0, PyTorch has all needed optimizations for Intel CPU and XPU.
/opt/conda/envs/linc/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Selected Tasks: ['folio-baseline-1shot']
Loading the model and tokenizer from HF (in fp32)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.31s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.06s/it]
/opt/conda/envs/linc/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:999: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
number of problems for this task is 5
  0%|          | 0/5 [00:00<?, ?it/s] 20%|██        | 1/5 [00:06<00:25,  6.37s/it] 40%|████      | 2/5 [00:06<00:08,  2.93s/it] 60%|██████    | 3/5 [00:08<00:04,  2.45s/it] 80%|████████  | 4/5 [00:13<00:03,  3.47s/it]100%|██████████| 5/5 [00:15<00:00,  2.78s/it]100%|██████████| 5/5 [00:15<00:00,  3.08s/it]
Error in parsing and/or evaluating LLM output: Increase `--max_length_generation` to avoid truncation
Error in parsing and/or evaluating LLM output: Increase `--max_length_generation` to avoid truncation
Error in parsing and/or evaluating LLM output: Increase `--max_length_generation` to avoid truncation
Error in parsing and/or evaluating LLM output: Increase `--max_length_generation` to avoid truncation
raw generations were saved
processed generations were saved
references were saved
{
  "config": {
    "model": "mistralai/Mistral-7B-v0.1"
  },
  "folio-baseline-1shot": {
    "accuracy (pass@1 majority)": 0.0
  }
}
